{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA-yzP01qTbo",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "222b5ebc-ee8a-4786-eaac-b175e04fda7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/database\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-w5iuyMYw4sBh7zsWfhCJTEyV2fhisyw\n",
            "From (redirected): https://drive.google.com/uc?id=1-w5iuyMYw4sBh7zsWfhCJTEyV2fhisyw&confirm=t&uuid=d97571e2-07d2-492a-be05-67997a120c61\n",
            "To: /content/database/data.zip\n",
            "100% 3.81G/3.81G [00:53<00:00, 71.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unzipping: 100%|██████████| 285497/285497 [01:00<00:00, 4745.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir database\n",
        "%cd database\n",
        "!gdown 1-w5iuyMYw4sBh7zsWfhCJTEyV2fhisyw\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
        "    for file in tqdm(zip_ref.namelist(), desc='Unzipping'):\n",
        "        zip_ref.extract(file)\n",
        "\n",
        "!rm data.zip\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "GKNsQzW0AA2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeeaa7b1-6a4d-4c06-d09d-3ab3a1d956e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m898.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.2/591.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for contexttimer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.16 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\n",
            "albumentations 1.4.15 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.5.5.64 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import math\n",
        "import googletrans\n",
        "import translate\n",
        "import underthesea\n",
        "from pyvi import ViUtils, ViTokenizer\n",
        "from difflib import SequenceMatcher\n",
        "from langdetect import detect\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import re\n",
        "from fuzzywuzzy import process as fuwu_process, fuzz as fuwu_fuzz\n",
        "from rapidfuzz import process as rafu_process, fuzz as rafu_fuzz\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "# -------------------------- #\n",
        "#        Session State       #\n",
        "# -------------------------- #\n",
        "\n",
        "# Initialize session state variables\n",
        "if 'expander_content' not in st.session_state:\n",
        "    st.session_state['expander_content'] = None\n",
        "\n",
        "if 'copy_to_clipboard' not in st.session_state:\n",
        "    st.session_state['copy_to_clipboard'] = None\n",
        "\n",
        "if 'selected_images' not in st.session_state:\n",
        "    st.session_state['selected_images'] = {}\n",
        "\n",
        "if 'checkbox_states' not in st.session_state:\n",
        "    st.session_state['checkbox_states'] = {}\n",
        "\n",
        "if 'search_results' not in st.session_state:\n",
        "    st.session_state['search_results'] = None\n",
        "\n",
        "# -------------------------- #\n",
        "#    Data Loading Functions   #\n",
        "# -------------------------- #\n",
        "\n",
        "@st.cache_resource\n",
        "def load_data_and_models():\n",
        "    # Load keyframes\n",
        "    lst_keyframes = glob.glob('database/s_optimized_keyframes/*.webp')\n",
        "    lst_keyframes.sort()\n",
        "\n",
        "    id2img_fps = {i: img_path for i, img_path in enumerate(lst_keyframes)}\n",
        "    print(f\"Total keyframes loaded: {len(id2img_fps)}\")\n",
        "\n",
        "    # Load video URLs\n",
        "    with open('database/vid_url.json', 'r') as f:\n",
        "        vid_url = json.load(f)\n",
        "    print(f\"Total videos loaded: {len(vid_url)}\")\n",
        "\n",
        "    # Load FPS data\n",
        "    with open('database/url_fps.json', 'r') as f:\n",
        "        url_fps = json.load(f)\n",
        "    print(f\"Total FPS data loaded: {len(url_fps)}\")\n",
        "\n",
        "    keyframes = pd.read_csv('database/keyframes.csv', sep='|', index_col=False)\n",
        "\n",
        "    # Load models\n",
        "    from transformers import CLIPModel, CLIPImageProcessor, CLIPTokenizer\n",
        "    from lavis.models import load_model_and_preprocess\n",
        "\n",
        "    clip_model = [\n",
        "        (\"openai/clip-vit-base-patch32\", 'clipB32'),\n",
        "        # Add more models if needed\n",
        "    ]\n",
        "\n",
        "    blip_model = (\"blip2_feature_extractor\", \"blip2fe\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Device in use: {device}')\n",
        "\n",
        "    # Load BLIP model and preprocessors\n",
        "    model, vis_processors, txt_processors = load_model_and_preprocess(\n",
        "        name=blip_model[0],\n",
        "        model_type=\"pretrain\",\n",
        "        is_eval=True,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Initialize CLIP models\n",
        "    models = [CLIPModel.from_pretrained(model_name).to(device) for model_name, _ in clip_model]\n",
        "    models.append(model.to(device))  # Append BLIP model\n",
        "\n",
        "    # Initialize image processors\n",
        "    image_processors = [CLIPImageProcessor.from_pretrained(model_name) for model_name, _ in clip_model]\n",
        "    image_processors.append(vis_processors)  # Append BLIP image processor\n",
        "\n",
        "    # Initialize text processors\n",
        "    text_processors = [CLIPTokenizer.from_pretrained(model_name) for model_name, _ in clip_model]\n",
        "    text_processors.append(txt_processors)  # Append BLIP text processor\n",
        "\n",
        "    return lst_keyframes, id2img_fps, vid_url, url_fps, keyframes, device, models, image_processors, text_processors, clip_model, blip_model\n",
        "\n",
        "# Load data and models\n",
        "with st.spinner('Loading database and models...'):\n",
        "    lst_keyframes, id2img_fps, vid_url, url_fps, keyframes, device, models, image_processors, text_processors, clip_model, blip_model = load_data_and_models()\n",
        "# st.success('All data and models have been loaded successfully!')\n",
        "\n",
        "# -------------------------- #\n",
        "#      Helper Classes        #\n",
        "# -------------------------- #\n",
        "\n",
        "class Translation:\n",
        "    def __init__(self, from_lang='vi', to_lang='en', mode='googletrans'):\n",
        "        self.__mode = mode\n",
        "        self.__from_lang = from_lang\n",
        "        self.__to_lang = to_lang\n",
        "\n",
        "        if mode == 'googletrans':\n",
        "            self.translator = googletrans.Translator()\n",
        "        elif mode == 'translate':\n",
        "            self.translator = translate.Translator(from_lang=from_lang, to_lang=to_lang)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported translation mode: {mode}\")\n",
        "\n",
        "    def preprocessing(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = self.preprocessing(text)\n",
        "        if self.__mode == 'translate':\n",
        "            return self.translator.translate(text)\n",
        "        else:\n",
        "            return self.translator.translate(text, dest=self.__to_lang).text\n",
        "\n",
        "class Text_Preprocessing:\n",
        "    def __init__(self, stopwords_path='./dict/vietnamese-stopwords-dash.txt'):\n",
        "        with open(stopwords_path, 'rb') as f:\n",
        "            lines = f.readlines()\n",
        "        self.stop_words = [line.decode('utf8').strip() for line in lines]\n",
        "\n",
        "    def remove_stopwords(self, text):\n",
        "        text = ViTokenizer.tokenize(text)\n",
        "        return \" \".join([w for w in text.split() if w not in self.stop_words])\n",
        "\n",
        "    def lowercasing(self, text):\n",
        "        return text.lower()\n",
        "\n",
        "    def text_norm(self, text):\n",
        "        return underthesea.text_normalize(text)\n",
        "\n",
        "    def text_classify(self, text):\n",
        "        return underthesea.classify(text)\n",
        "\n",
        "    def __call__(self, text):\n",
        "        text = self.lowercasing(text)\n",
        "        text = self.remove_stopwords(text)\n",
        "        text = self.text_norm(text)\n",
        "        categories = self.text_classify(text)\n",
        "        return categories\n",
        "\n",
        "@st.cache_data\n",
        "def load_stopwords():\n",
        "    return Text_Preprocessing()\n",
        "\n",
        "class Myfaiss:\n",
        "    def __init__(self, bin_file: str, id2img_fps, device, model, text_processor, image_processor, vid_url, url_fps):\n",
        "        self.index = self.load_bin_file(bin_file)\n",
        "        self.id2img_fps = id2img_fps\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.text_processor = text_processor\n",
        "        self.image_processor = image_processor\n",
        "        self.vid_url = vid_url\n",
        "        self.url_fps = url_fps\n",
        "\n",
        "    def load_bin_file(self, bin_file: str):\n",
        "        return faiss.read_index(bin_file)\n",
        "\n",
        "    def show_images(self, image_paths):\n",
        "        num_cols = 5  # Adjust as needed\n",
        "        rows = [image_paths[i:i + num_cols] for i in range(0, len(image_paths), num_cols)]\n",
        "\n",
        "        for row_idx, row in enumerate(rows):\n",
        "            cols = st.columns(len(row))\n",
        "            for idx, img_path in enumerate(row):\n",
        "                vid_id = os.path.basename(img_path).split('.')[0]\n",
        "                vid_name, frame = vid_id.split('-')\n",
        "                fps = self.url_fps.get(self.vid_url.get(vid_name, \"\"), 1)\n",
        "                timestamp = int(int(frame) / fps)\n",
        "                video_url = f\"{self.vid_url.get(vid_name, '')}&t={timestamp}\"\n",
        "\n",
        "                # Ensure image_id is unique by adding row and column indices\n",
        "                image_id = f\"{vid_name}_{frame}_{row_idx}_{idx}\"\n",
        "                checkbox_key = f\"checkbox_{image_id}\"\n",
        "\n",
        "                # Initialize checkbox state if not present\n",
        "                if image_id not in st.session_state['checkbox_states']:\n",
        "                    st.session_state['checkbox_states'][image_id] = False\n",
        "\n",
        "                with cols[idx]:\n",
        "                    try:\n",
        "                        st.image(img_path, width=150)  # Set fixed width\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error loading image: {e}\")\n",
        "\n",
        "                    # Center the button and checkbox using HTML and CSS\n",
        "                    st.markdown(\n",
        "                        f\"\"\"\n",
        "                        <div style=\"display:flex; flex-direction: column; align-items: center;\">\n",
        "                        \"\"\",\n",
        "                        unsafe_allow_html=True,\n",
        "                    )\n",
        "\n",
        "                    # Define the callback function\n",
        "                    def button_callback(vid_name=vid_name, frame=frame, video_url=video_url, button_label=f\"{vid_name}, {frame}\"):\n",
        "                        st.session_state['expander_content'] = (vid_name, frame, video_url)\n",
        "                        st.session_state['copy_to_clipboard'] = button_label\n",
        "\n",
        "                    st.button(\n",
        "                        f\"{vid_name}, {frame}\",\n",
        "                        key=f\"btn_{image_id}\",\n",
        "                        on_click=button_callback\n",
        "                    )\n",
        "\n",
        "                    # Checkbox for selection\n",
        "                    selected = st.checkbox(\"Select\", key=checkbox_key)\n",
        "                    st.session_state['checkbox_states'][image_id] = selected\n",
        "\n",
        "                    # Update selected_images\n",
        "                    if selected:\n",
        "                        st.session_state['selected_images'][image_id] = (vid_name, frame, img_path)\n",
        "                    else:\n",
        "                        st.session_state['selected_images'].pop(image_id, None)\n",
        "\n",
        "                    st.markdown(\n",
        "                        \"</div>\",\n",
        "                        unsafe_allow_html=True,\n",
        "                    )\n",
        "\n",
        "    def image_search(self, id_query, k, bin_file):\n",
        "        query_feats = self.index.reconstruct(id_query).reshape(1, -1)\n",
        "        scores, idx_image = self.index.search(query_feats, k=k)\n",
        "        idx_image = idx_image.flatten()\n",
        "\n",
        "        infos_query = list(map(self.id2img_fps.get, list(idx_image)))\n",
        "        image_paths = [info for info in infos_query]\n",
        "\n",
        "        return scores, idx_image, infos_query, image_paths\n",
        "\n",
        "    def text_search(self, text, k):\n",
        "        translater = Translation()\n",
        "        if detect(text) == 'vi':\n",
        "            text = translater(text)\n",
        "\n",
        "        ###### TEXT FEATURES EXTRACTING ######\n",
        "        if self.model == models[-1]:  # Assuming BLIP model is the last\n",
        "            text_input = self.text_processor[\"eval\"](text)\n",
        "            sample = {\"text_input\": [text_input]}\n",
        "            features_text = self.model.extract_features(sample, mode=\"text\")\n",
        "            text_features = features_text.text_embeds_proj[:, 0, :].detach().cpu().numpy()\n",
        "        else:\n",
        "            inputs = self.text_processor(text, return_tensors=\"pt\").to(self.device)\n",
        "            with torch.no_grad():\n",
        "                text_features = self.model.get_text_features(**inputs).cpu().detach().numpy().astype(np.float32)\n",
        "\n",
        "        ###### SEARCHING #####\n",
        "        scores, idx_image = self.index.search(text_features, k=k)\n",
        "        idx_image = idx_image.flatten()\n",
        "\n",
        "        ###### GET INFOS KEYFRAMES_ID ######\n",
        "        infos_query = list(map(self.id2img_fps.get, list(idx_image)))\n",
        "        image_paths = [info for info in infos_query]\n",
        "\n",
        "        return scores, idx_image, infos_query, image_paths\n",
        "\n",
        "    def image_similarity_search(self, image_path, k, online=False):\n",
        "        if online:\n",
        "            import requests\n",
        "            img = Image.open(requests.get(image_path, stream=True).raw).convert('RGB')\n",
        "        else:\n",
        "            img = Image.open(image_path)\n",
        "\n",
        "        if self.model == models[-1]:  # Assuming BLIP model is the last\n",
        "            image = self.image_processor[\"eval\"](img).unsqueeze(0).to(self.device)\n",
        "            sample = {\"image\": image}\n",
        "            with torch.no_grad():\n",
        "                features_image = self.model.extract_features(sample, mode=\"image\")\n",
        "                image_features = features_image.image_embeds_proj[:, 0, :].detach().cpu().numpy()\n",
        "        else:\n",
        "            inputs = self.image_processor(images=img, return_tensors=\"pt\").to(self.device)\n",
        "            with torch.no_grad():\n",
        "                image_features = self.model.get_image_features(**inputs).detach().cpu().numpy()\n",
        "\n",
        "        scores, idx_image = self.index.search(image_features, k=k)\n",
        "        idx_image = idx_image.flatten()\n",
        "        infos_query = list(map(self.id2img_fps.get, list(idx_image)))\n",
        "        image_paths = [info for info in infos_query]\n",
        "        return scores, idx_image, infos_query, image_paths\n",
        "\n",
        "# -------------------------- #\n",
        "#        FAISS Search        #\n",
        "# -------------------------- #\n",
        "\n",
        "# Define root features path\n",
        "root_features = 'database/features'\n",
        "\n",
        "# Prepare bin paths\n",
        "bin_paths = [os.path.join(root_features, bin_name + '.bin') for _, bin_name in clip_model]\n",
        "bin_paths.append(os.path.join(root_features, blip_model[1] + '.bin'))\n",
        "\n",
        "# Ensure that the number of bin_paths matches number of models\n",
        "if len(bin_paths) != len(models):\n",
        "    st.error(\"Number of bin paths does not match number of models.\")\n",
        "    st.stop()\n",
        "\n",
        "# Initialize faiss_search\n",
        "faiss_search = [Myfaiss(bin_paths[i], id2img_fps, device, models[i], text_processors[i], image_processors[i], vid_url, url_fps) for i in range(len(models))]\n",
        "\n",
        "# -------------------------- #\n",
        "#           Main             #\n",
        "# -------------------------- #\n",
        "\n",
        "def main():\n",
        "    # Title with gradient and centered\n",
        "    st.markdown(\"\"\"\n",
        "        <h1 style='text-align: center; background: linear-gradient(to right, blue, purple); -webkit-background-clip: text; color: transparent;'>Image Retrieval System - AIC2024</h1>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    col1, col2 = st.columns([1, 2])\n",
        "\n",
        "    with col1:\n",
        "        # Video Details Expander\n",
        "        video_details_expander = st.expander(\"Video details\")\n",
        "        with video_details_expander:\n",
        "            if st.session_state['expander_content']:\n",
        "                vid_name, frame, video_url = st.session_state['expander_content']\n",
        "                st.video(video_url)\n",
        "                st.write(f\"**Video ID:** {vid_name}, {frame}\")\n",
        "                st.write(f\"**Video URL:** {video_url}\")\n",
        "\n",
        "                # Checkbox in video details\n",
        "                image_id = f\"{vid_name}_{frame}\"\n",
        "                checkbox_key = f\"checkbox_{image_id}\"\n",
        "                if image_id not in st.session_state['checkbox_states']:\n",
        "                    st.session_state['checkbox_states'][image_id] = False\n",
        "\n",
        "                selected = st.checkbox(\"Select\", key=checkbox_key)\n",
        "                st.session_state['checkbox_states'][image_id] = selected\n",
        "\n",
        "                # Update selected_images\n",
        "                if selected:\n",
        "                    # Find img_path from id2img_fps\n",
        "                    img_path = None\n",
        "                    for path in id2img_fps.values():\n",
        "                        if image_id in path:\n",
        "                            img_path = path\n",
        "                            break\n",
        "                    if img_path:\n",
        "                        st.session_state['selected_images'][image_id] = (vid_name, frame, img_path)\n",
        "                else:\n",
        "                    st.session_state['selected_images'].pop(image_id, None)\n",
        "            else:\n",
        "                st.write(\"No video selected.\")\n",
        "\n",
        "        # Selected Images Expander\n",
        "        selected_images_expander = st.expander(\"Selected image(s)\")\n",
        "        with selected_images_expander:\n",
        "            selected_images = st.session_state['selected_images'].values()\n",
        "            if selected_images:\n",
        "                for vid_name, frame, img_path in selected_images:\n",
        "                    st.write(f\"**{vid_name}, {frame}**\")\n",
        "            else:\n",
        "                st.write(\"No images selected.\")\n",
        "\n",
        "    with col2:\n",
        "        # ------------------------------ #\n",
        "        #    Added Slider and Checkbox   #\n",
        "        # ------------------------------ #\n",
        "\n",
        "        # Slider for K_neighbors\n",
        "        K_neighbors = st.slider(\n",
        "            \"Number of Neighbors (K_neighbors)\",\n",
        "            min_value=10,\n",
        "            max_value=1000,\n",
        "            value=100,\n",
        "            step=10,\n",
        "            help=\"Adjust the number of nearest neighbors to retrieve.\"\n",
        "        )\n",
        "\n",
        "        # Checkbox for high_performance\n",
        "        if st.checkbox(\n",
        "            \"Use High Performance Mode\",\n",
        "            value=False,\n",
        "            help=\"Toggle to use high performance search mode.\"\n",
        "        ):\n",
        "            high_performance = 1\n",
        "        else:\n",
        "            high_performance = 0\n",
        "\n",
        "        # Search bar\n",
        "        text_query = st.text_input(\"Enter a text query, a frame or an image url\", placeholder='Eg: \"Cảnh quay một chiếc thuyền cứu hộ đi trên băng...\" || \"L01_V001, 1\" || \"https://bitexco.c...scaled.jpg\"', key=\"text_query\")\n",
        "        search_clicked = st.button(\"Search\", key=\"search_button\")\n",
        "\n",
        "        if search_clicked and text_query:\n",
        "            # Determine the model index based on high_performance\n",
        "            if high_performance and len(faiss_search) > 1:\n",
        "                search_index = 1  # Assuming the second model is for high performance\n",
        "            else:\n",
        "                search_index = 0  # Default to the first model\n",
        "\n",
        "            with st.spinner('Performing search...'):\n",
        "                if \"https://\" in text_query:\n",
        "                    scores, idx_image, infos_query, image_paths = faiss_search[search_index].image_similarity_search(text_query, k=K_neighbors, online=True)\n",
        "                elif re.match(r'^L\\d{2}_V\\d{3},\\s*(\\d|[1-9]\\d{0,4})$', text_query):\n",
        "                    ROOT_IMG = \"database/s_optimized_keyframes\"\n",
        "\n",
        "                    input_vid_name, input_frame = text_query.split(', ')\n",
        "                    input_frame = int(input_frame)\n",
        "                    filtered_df = keyframes[(keyframes['vid_name'] == input_vid_name) & (keyframes['shot'].apply(lambda x: eval(x)[0] <= input_frame <= eval(x)[1]))]\n",
        "                    closest_row = filtered_df.iloc[(filtered_df['frame'] - input_frame).abs().argsort()[:1]]\n",
        "                    text_query = f\"{closest_row['vid_name'].values[0]}, {str(closest_row['frame'].values[0]).zfill(5)}\"\n",
        "\n",
        "                    image_path = os.path.join(ROOT_IMG, '-'.join(text_query.split(', ')) + '.webp')\n",
        "                    print(image_path)\n",
        "                    scores, idx_image, infos_query, image_paths = faiss_search[search_index].image_similarity_search(image_path, k=K_neighbors)\n",
        "                else:\n",
        "                    # Perform the search\n",
        "                    scores, idx_image, infos_query, image_paths = faiss_search[search_index].text_search(text_query, k=K_neighbors)\n",
        "                # Store results in session_state\n",
        "                st.session_state['search_results'] = image_paths\n",
        "                # Reset checkbox states for new search\n",
        "                st.session_state['checkbox_states'] = {}\n",
        "                # Reset selected_images for new search\n",
        "                st.session_state['selected_images'] = {}\n",
        "\n",
        "\n",
        "        # Display images from session_state if available\n",
        "        if st.session_state.get('search_results'):\n",
        "            # Determine the model index for displaying based on high_performance\n",
        "            if high_performance and len(faiss_search) > 1:\n",
        "                display_index = 1\n",
        "            else:\n",
        "                display_index = 0\n",
        "\n",
        "            with st.spinner('Loading images...'):\n",
        "                faiss_search[display_index].show_images(st.session_state['search_results'])\n",
        "\n",
        "    # Handle copying to clipboard\n",
        "    if st.session_state['copy_to_clipboard']:\n",
        "        js_button_label = json.dumps(st.session_state['copy_to_clipboard'])\n",
        "        js_code = f\"\"\"\n",
        "        <script>\n",
        "        navigator.clipboard.writeText({js_button_label});\n",
        "        </script>\n",
        "        \"\"\"\n",
        "        st.markdown(js_code, unsafe_allow_html=True)\n",
        "        # Reset copy_to_clipboard after copying\n",
        "        st.session_state['copy_to_clipboard'] = None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "21HtXilerXQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877a34ed-a350-4365-fd98-5bebd9a086b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "zQJciCJhrpq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -q localtunnel\n",
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "HYCCSzIXr0Fn",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "198ebf34-3ad2-498f-fc73-e9d8d5895d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 1s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" password\")\n",
        "print(\"     |\")\n",
        "print(\"     V\")\n",
        "!wget -q -O - ipv4.icanhazip.com\n",
        "!streamlit run app.py & npx localtunnel --port 8501 --subdomain aicretrievalsystem"
      ],
      "metadata": {
        "id": "TDTnoBd90LZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5a578e-7184-467f-a69d-5f2194424e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " password\n",
            "     |\n",
            "     V\n",
            "35.203.187.102\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.203.187.102:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://aicretrievalsystem.loca.lt\n",
            "Total keyframes loaded: 285492\n",
            "Total videos loaded: 726\n",
            "Total FPS data loaded: 726\n",
            "2024-10-12 14:47:04.591080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-12 14:47:04.851632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-12 14:47:04.923923: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-12 14:47:05.356015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-12 14:47:08.245152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore\n",
            "/usr/local/lib/python3.10/dist-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore\n",
            "Device in use: cuda\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/lavis/models/eva_vit.py:433: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(cached_file, map_location=\"cpu\")\n",
            "/usr/local/lib/python3.10/dist-packages/lavis/models/blip2_models/blip2.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(cached_file, map_location=\"cpu\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:415: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
            "database/s_optimized_keyframes/L04_V020-27570.webp\n"
          ]
        }
      ]
    }
  ]
}